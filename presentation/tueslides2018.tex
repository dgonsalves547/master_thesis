\documentclass[t,aspectratio=169]{beamer}
\usetheme[calibri=false]{tue2018}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{booktabs}

\setbeameroption{show notes on second screen=right}

\makeatletter

\graphicspath{{../thesis}}
\usepackage[english]{babel}

\title{Variational Auto-encoding and Segmentation}
% \subtitle{Subtitle of Presentation}
\author{H.J.M. van Genuchten, Master Student DS\&AI}
\date{2024-09-19}
\department{Mathematics and Computer Science, Generative Artificial Intelligence}


\begin{document}

\setbeamertemplate{caption}{\raggedright\insertcaption\par}


\definecolor{avularred}{HTML}{EE1E47}
\company{avularlogo}{avularred}
\renewcommand{\companylogosizemultiplicationfactor}{2}


\begin{titleframe}[variant=1,bgimage=titlebgimg.jpg]
\end{titleframe}

\title{Variational Auto-encoding and Segmentation}

\setbeamercolor{normal text}{fg=tuedarkblue}
\usebeamercolor{normal text}

\begin{frame}
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Table of Contents}
  \begin{enumerate}
    \item Introduction
    \item Background Knowledge
    \item Our Method
    \item Experiments and Results
    \item Discussion
    \item Conclusion
  \end{enumerate}
  \note[item]{I will start with a short introduction of the thesis, there I will shortly introduce you to the problem at hand, and the current limitations we need to take into account. I will also state the RQs.}
  \note[item]{Then, I will give you a quick summary of the necessary background knowledge required}
  \note[item]{After which, I will present you our method}
  \note[item]{Followed by the experiments we did and the results we got from them}
  \note[item]{Finally, I will give you our conclusion about our method, as well as a few more general takeaways we came across.}
\end{frame}

\begin{frame}
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  \frametitle{Introduction}
  The problems:
  \begin{itemize}
    \item Navigation for robotic devices.
          \pause
    \item Computer Vision.
          \pause
    \item On edge.
          \pause
    \item New and dynamic environments.
  \end{itemize}
  \note<1-2>[item]{Within mobile robotics, one of the important factors is navigation. As an example we can take an automated warehouse, in which the robot needs to pick up a package from location A and then needs to bring that to a drop-off point B.}
  \note<1-3>[item]{For this navigation we need to robot to be able to see its environment, hence we come to computer vision. Current techniques rely on massive amounts of labelled data to be able to understand our world.}
  \note<2-4>[item]{One caveat, is that we want the robot to be deployed in new environments without a complex onboarding process. Current machine learning algorithms have shown that although they can be reliable when the data they encounter in production is similar to that which they encountered during training, they can be unreliable when they see things they have never seen before. One solution would be to show them all the things.}\
  \note<3-4>[item]{However, that brings us to the second caveat, all the calculations that are needed to navigate the robot, ideally should happen on the device itself. As otherwise it limits the situations in which it can be deployed and as we cannot strap a datacenter on the back of the robot, we need to employ an efficient network, which cannot be too big.}
  \note<3-4>[item]{Another caveat: Model needs to train quickly}
\end{frame}

\begin{frame}
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Introduction}
  Research Questions:
  \begin{enumerate}
    \pause
    \item Can the VAE be adapted to an Image Segmentation task?
          \pause
    \item Do the pre-trained weights of a VAE provide useful features for image segmentation?
          \pause
    \item Does pre-training on the VAE task reduce the amount of labelled data required?
  \end{enumerate}
  \note[item]{Can the VAE be adapted to an Image Segmentation task?}
  \note[item]{Do the pre-trained weights of a VAE provide useful features for image segmentation?}
  \note[item]{Does pre-training on the VAE task reduce the amount of labelled data required?}
\end{frame}

\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Background Knowledge}
  \framesubtitle{SLAM}
  \begin{itemize}
    \item Simultaneous \alert<1>{Localization} and \alert<1>{Mapping}
          \pause
    \item Landmarks
          \pause
    \item Semantic-SLAM
  \end{itemize}
  
  \note{SLAM tries to solve one of the fundamental problems in robotics. Namely, answering the questions: Where am I?, and How does the world look like?}
  \note[item]{To navigate from A to B you need a map.}
  \note[item]{Who am I? + What does the world look like?}
  \note[item]{Landmarks - Slam uses landmarks to associate its current relative position to that on its internal representation on the MAP. However, how do you know for sure that a landmark is the same as the one you saw earlier, and that the landmark itself did not move?}
  \note[item]{Semantic-SLAM - You can collect more data about each landmark such as the object type the landmark belongs to. I.e., the semantic value of the object.}
\end{frame}

\begin{frame}
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Background Knowledge}
  \framesubtitle{SLAM}
  \begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{images/slam_example.png}
  \end{figure}
  \note{The map that is generated by a SLAM algorithm can be visualised as well as the robot, here is such an example. As you can see it is not perfect, this is because the sensors used to collect the data are not perfect and subject to noise.}
\end{frame}

\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Background Knowledge}
  \framesubtitle{\alert<3>{Semantic} \alert<2>{Segmentation}}
  
  \begin{itemize}
    \pause
    \item Segmentation: Division into separate parts.
          \pause
    \item Semantic: The study of a meaning.
  \end{itemize}
  \pause
  Determine for each pixel in an image the class of the object the pixel belongs to.
  
  \begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/datasets/coco/samples/val/1-t.png}
    \includegraphics[width=0.49\textwidth]{figures/datasets/coco/samples/val/0-t.png}
  \end{figure}
  
  \note<1>{Semantic slam uses a machine learning algorithm called: Semantic Segmentation}
  \note<2>{Segmentation is the division into separate groups, in the case of images it is the grouping of pixels.}
  \note<3>{Semantic, is the study of meaning. In this context, we want to group the pixels based on the meaning we associate to it. I.e. the object type or class}
  \note<4>{I.e., we want to determine for each pixel the class of the object it belongs to. Here are two examples, on the left there is a simple scene, and on the right a more complex scene.}
\end{frame}

\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Background Knowledge}
  \framesubtitle{Semantic Segmentation -- Existing Networks}
  
  \begin{minipage}[b]{0.49\textwidth}
    \vspace{0pt}
    \begin{figure}
      \includegraphics[width=0.8\textwidth]{figures/unet-architecture-t.png}
      \caption{Architecture of the U-Net}
    \end{figure}
  \end{minipage}
  \hfill
  \pause
  \begin{minipage}[b]{0.49\textwidth}
    \vspace{0pt}
    \begin{figure}
      \includegraphics[width=1\textwidth]{figures/fpn-architecture-t.png}
      \caption{Architecture of the Feature Pyramid Network (FPN)}
    \end{figure}
  \end{minipage}%
  
  \note<1>{The U-Net is a simple architecture which first downsamples the image using various convolutional layers, and then subsequently upsamples the layers. It has additional `skip' connections that provide information from higher up the encoder to the decoder part of the network. It gets its name from the shape of the network.}
  \note<2>{The Feature Pyramid Network is quite similar to the U-Net, but instead of only upsampling it create multiple prediction layers which are later combined. The hidden layers are slightly smaller due to which the entire model is slightly faster and in our experiments slightly better. The main drawback of these and other semantic segmentation networks is, is that they require a lot of labeled data, and this labeling is labour intensive and thus expensive. This can be mitigated somewhat by using part of an other network that is trained on a different task. For example classifying an entire image. This is also called pretraining.}
\end{frame}

\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Background Knowledge}
  \framesubtitle{Variational Auto-encoding -- General Idea}
  
  \begin{itemize}
    \pause
    \item An image is generated based on a random process with hidden parameters $\theta$. \\
          $p_\theta(x)$ \pause
    \item This process is conditional on another (simpler) random process. \\
          $p_\theta(x|z)$ \pause
    \item  We assume that $p_\theta(z)$ is a `simple' probability distribution.
          \pause
    \item Thus, if we can find a function that approximates $p_\theta(x|z)$ we can generate new images.
          \pause
    \item Problem: We do not know $p_\theta(x | z)$, nor the $z$ associated to $x$.
          \pause
    \item Solution: Learn $q(z|x)$
  \end{itemize}
  
  \note<1>{Then the Varitional Auto-encoding, or VAE. I will first explain the general Idea behind it. In its classic form one of its main task is generating new images. Thus: we assume that}
  \note<2>{Thus: we assume that the images we are interested in are generated by a random process which can be parameterized with theta. These parameters are unknown.}
  \note<3>{We assume that this process is conditional on a different random process, that of $p_\theta(z)$, thus we have the conditional probability of $p_\theta(x | z)$}
  \note<4>{Assume $p_\theta(z)$ is simple and that we can sample from it easily.}
  \note<5>{Thus If we can find the parameters of $p_\theta(x|z)$. We can generate novel images based of samples from our simple distribution.}
  \note<6>{Sadly this is difficult, and for machine learning we need to know the z associated to a specific x to be able to learn this process.}
  \note<7>{We will try to learn $q_\phi(z|x)$}
  
\end{frame}

\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Background Knowledge}
  \framesubtitle{Variational Auto-encoding -- ELBO}
  
  \begin{minipage}[t]{0.7\textwidth}
    \vspace{0pt}
    \begin{itemize}
      \item Encoder: $q_{\phi}(z|x)$
      \item Prior: $p_\theta(z)$
      \item Decoder: $p_\theta(x|z)$
            \pause
      \item $\mathcal{L}_{ELBO} = \alert<3>{\mathbb{E}_{q_{\phi}(z|x)}[\log p_\theta(x|z)]} - \alert<5>{\beta} \cdot \alert<4>{D_{KL}(q_{\phi}(z|x) || p_\theta(z))}$
    \end{itemize}
  \end{minipage}%
  \hfill
  \begin{minipage}[t]{0.3\textwidth}
    \vspace{0pt}
    \includegraphics[width=1\textwidth]{figures/vae.png}
  \end{minipage}
  
  \note<1>{The VAE does this using 3 distributions, the `encoder', `prior', and `dedocer'. The encoder and decoder are approximated by a NN, and p(z) is a simple distribution, e.g. Normal distribution}
  \note<2>{They are optimized by the evidence lower bound (ELBO). Which consists of 2 main parts. The reconstruction part and the KL-Divergence.}
  \note<3>{The reconstruction part aims to ensure that $x$ and $x$ prime are as similar as possible.}
  \note<4>{The KL-Divergence aims to ensure that the output of the encoder is similar to our simple prior. Thus allowing us to sample from this simple distribution and create a image that should be similar to the images from our dataset.}
  \note<5>{This is an extension to the classic VAE, which allows one to control the loss a bit better}
\end{frame}

\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/beta-vae/samples/Imagenet-b0.01/20.png}
    \includegraphics[width=0.49\textwidth]{figures/beta-vae/samples/Imagenet-b0.01/21.png}
  \end{figure}
  \note{Here are some examples of the VAE reconstructing images based of an image. The left part is the original image and the right part is the reconstructed image. }
\end{frame}

\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Background Knowledge}
  \framesubtitle{Variational Auto-encoding -- Summarizing}
  
  The VAE is interested in the following processes:
  \begin{tabular}{ll}
    $p(z)$              & The `hidden' generative process of the object.                      \\
    $p(x | z)$          & The generative process of observation of the object.                \\
    $p(z | x)$          & The inverse of the generative process of observation of the object. \\
    $p(x|z) \cdot p(z)$ & The full generative process of images and `hidden' process.
  \end{tabular}
  
  \note{Summarizing the VAE is intereseted in these distributions.}
  
\end{frame}

\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Method}
  \framesubtitle{Extending the VAE}
  
  \begin{tabular}{ll}
    VAE               & $p(x, z) = p(x | z) \cdot p(z)$                      \\
    \pause
    VAES (our method) & $p(x, y, z) = p(y | x, z) \cdot p(x | z) \cdot p(z)$ \\
  \end{tabular}
  
  \note<1>{Recall the distribution the VAE is approximating, with $x$ being an image and $z$ the prior.}
  \note<2>{We extend this to also include $y$ the semantic label of the image. Notice that a large part of our model is equal to the VAE. Thus we can possibly first train the VAE part, which could reduce the amount of labelled data we need to achieve the same results as we already have a learnt a representation of the data.}
\end{frame}


\begin{frame}[fragile]
  \setbeamercolor{normal text}{fg=tuedarkblue}
  \usebeamercolor[fg]{normal text}
  
  \frametitle{Method}
  \framesubtitle{VAES -- Parts}
  
  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item<1-> Encoder: $q_{\phi}(z|x)$
        \item<1-> Prior: $p_\theta(z)$
        \item<1-> Image Decoder: $p_\theta(x|z)$
        \item<2-> Label Decoder: $p_\theta(y|x,z)$
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \vspace*{-1.4cm}
      \includegraphics<0->[width=0.7\textwidth]{figures/vaes.png}
      \vspace{10pt}
    \end{column}
  \end{columns}
  \pause
  \pause
  $\mathcal{L}_{ELBO_{label}} = \alert<3>{\mathbb{E}_{q_{\phi}(z | x)}[\log p_\psi(y | x, z)]} + \alert<4>{\gamma} \cdot \mathbb{E}_{q_{\phi}(z | x)}[\log p_\theta(x | z)] - \alert<4>{\beta} \cdot D_{KL}(q_{\phi}(z|x) || p_\theta(z))$
  
  \note<1>{Recall the VAE, we extend it with an extra decoder from the latent space $z$}
  \note<2>{We'll call this the label decoder. \alert{Skip connections}}
  \note<3>{This requires us to change the elbo to incorparate the reconstrution of the Semantic Label. This is done by adding this part. For the interested the full derivation can be found in the appendix of the thesis.}
  \note<4>{Added the gamma and beta to control the importance of each of the parts during training. This is mainly done so we have more control and can stabalize training if the parameters are chosen well.}
\end{frame}

\begin{frame}
  \frametitle{Method}
  \framesubtitle{Training Procdure}
  
  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      \begin{enumerate}
        \item<1-> Train the encoder and image decoder. \\
              $q_\phi(x | z)$ and $p(z | x)$
        \item<2-> Fine-tune the label decoder using the encoder. \\
              $p(y | x, z)$
      \end{enumerate}
    \end{column}
    \begin{column}{0.5\textwidth}
      \vspace*{-1.4cm}
      \includegraphics<0->[width=0.7\textwidth]{figures/vaes.png}
      \vspace{10pt}
    \end{column}
  \end{columns}
  \pause
  
  \note<1>{We start with training the encoder en decoder using the classic VAE method.}
  \note<2>{Then we fine-tune the label decoder using encoder trained earlier.}
\end{frame}

\begin{frame}
  \frametitle{Method}
  \framesubtitle{Metrics}
  \begin{subequations}
    \begin{align*}
      \text{True Positive (TP)}  & : \text{Correctly predicted the correct class} \\ 
      \text{False Positive (FP)} & : \text{Incorrectly predicting a class}        \\
      \text{False Negative (FN)} & : \text{Incorrectly missing a class}           \\
      \text{Jaccard Index}       & = \frac{TP}{FN + FP + TP}
    \end{align*}
  \end{subequations}
  
  \note[enumerate]{True Positive}
  \note[enumerate]{False Positive}
  \note[enumerate]{False Negative}
  \note[enumerate]{Jaccard index: Higher is better, 0 to 1}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \begin{enumerate}
    \item General Experimental Setup
          \pause
    \item Dataset
          \pause
    \item Comparison to Baselines Architectures
          \pause
    \item Reduction of Data Required
          \pause
    \item Model Characteristics
          \pause
    \item Ablation studies
  \end{enumerate}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{General Experimental Setup}
  \begin{itemize}
    \item Pytorch
          \pause
    \item 9 Hours
          \pause
    \item ResNet Backbone
          \pause
    \item Pretrained Weights
          \pause
    \item 5 Variational Skip Connections
  \end{itemize}
  
  \note[item]{Pytorch}
  \note[item]{8-10 Hours}
  \note[item]{ResNet Backbone}
  \note[item]{Pretrained Weights}
  \note[item]{5 Variational Skip Connections}
  
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{Dataset}
  \begin{itemize}
    \item CoCo
  \end{itemize}
  \vspace*{-2cm}
  \begin{figure}
    \centering
    \includegraphics<1->[width=0.6\textwidth]{figures/datasets/coco/class_distribution-t.png}
  \end{figure}
  
  \note[item]{CoCo Dataset}
  \note[item]{training 100k samples}
  \note[item]{133 classes to 27 to 25}
  \note[item]{Distribution}
\end{frame}

\begin{frame}
  \frametitle{Experimens \& Results}
  \framesubtitle{Comparison to Baselines}
  \begin{figure}
    \color{tuedarkblue}
    \centering
    \subfloat[Image]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/VAES-imagenet-False/11.png}}
    \subfloat[Ground Truth]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/VAES-imagenet-False/gt_11.png}}
    \subfloat[VAES-imagenet (Our Method)]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/VAES-imagenet-False/pr_11.png}}
    \subfloat[unet-imagenet]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/unet-imagenet-False/pr_11.png}}
    \subfloat[fpn-imagenet]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/fpn-imagenet-False/pr_11.png}}
  \end{figure}
  \note{Some samples of the various models using ImageNet as pretrained weights}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{Comparison to Baselines Architectures}
  \vspace*{-0.5cm}
  \begin{table}
    \centering
    \caption{The Evaluation Jaccard Index for our model and the baselines for various parameters. The higher the score, the better.}
    \color{black}
    \begin{tabular}{llrrrr}
      \toprule
                                & weights      & None          & imagenet      & vae-b1        & vae-b100      \\
      frozen                    & architecture &               &               &               &               \\
      \midrule
      \multirow[t]{3}{*}{False} & VAES         & 0.21          & 0.47          & 0.20          & 0.15          \\
                                & fpn          & \textbf{0.30} & \textbf{0.52} & \textbf{0.26} & \textbf{0.28} \\
                                & unet         & 0.23          & 0.49          & 0.21          & 0.24          \\
      \cline{1-6}
      \multirow[t]{3}{*}{True}  & VAES         & n.a.          & 0.40          & 0.20          & 0.18          \\
                                & fpn          & n.a.          & 0.45          & 0.21          & 0.21          \\
                                & unet         & n.a.          & 0.42          & 0.20          & 0.19          \\
      \cline{1-6}
      \bottomrule
    \end{tabular}
    
  \end{table}
  
  
  \note{Explain the Frozen, None, ImageNet and VAE weights}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{Comparison to Baselines Architectures}
  \vspace*{-0.5cm}
  \begin{table}
    \centering
    \caption{Coefficients of the OLS showing the influence of the hyperparameters on the Evaluation Jaccard Index.}
    \color{black}
    \begin{tabular}{lrrrrrr}
      \toprule
                           & Coef. & Std.Err. & t     & P>|t|         & [0.025 & 0.975] \\
      \midrule
      Intercept            & 0.22  & 0.02     & 10.54 & \textbf{0.00} & 0.17   & 0.26   \\
      weights[T.imagenet]  & 0.21  & 0.02     & 9.59  & \textbf{0.00} & 0.17   & 0.26   \\
      weights[T.vae-b1]    & -0.03 & 0.02     & -1.33 & 0.20          & -0.08  & 0.02   \\
      weights[T.vae-b100]  & -0.03 & 0.02     & -1.54 & 0.14          & -0.08  & 0.01   \\
      architecture[T.fpn]  & 0.06  & 0.02     & 3.49  & \textbf{0.00} & 0.02   & 0.09   \\
      architecture[T.unet] & 0.02  & 0.02     & 1.48  & 0.16          & -0.01  & 0.06   \\
      \bottomrule
    \end{tabular}
  \end{table}
  
  \note[item]{ANOVA}
  \note[item]{Significant values}
  \note[item]{Explain table}
  \note[item]{Intercept is the `base' model with No pretrained weights, VAES architecture}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{Reduction of Data Required}
  \vspace*{-0.5cm}
  \begin{table}[ht]
    \centering
    \color{black}
    \begin{tabular}{llrrrr}
      \toprule
                               & fraction & $1e^{0}$      & $1e^{-1}$     & $1e^{-2}$     & $1e^{-3}$     \\
      architecture             & weights  &               &               &               &               \\
      \midrule
      \multirow[t]{3}{*}{VAES} & None     & 0.15          & 0.16          & 0.18          & 0.12          \\
                               & imagenet & 0.34          & 0.37          & 0.39          & 0.21          \\
                               & vae-b10  & 0.14          & 0.16          & 0.18          & 0.12          \\
      \cline{1-6}
      \multirow[t]{3}{*}{fpn}  & None     & 0.22          & 0.12          & 0.21          & 0.17          \\
                               & imagenet & 0.45          & \textbf{0.48} & \textbf{0.48} & \textbf{0.34} \\
                               & vae-b10  & 0.19          & 0.21          & 0.23          & 0.16          \\
      \cline{1-6}
      \multirow[t]{3}{*}{unet} & None     & 0.21          & 0.10          & 0.18          & 0.14          \\
                               & imagenet & \textbf{0.46} & 0.44          & 0.43          & 0.30          \\
                               & vae-b10  & 0.20          & 0.18          & 0.21          & 0.14          \\
      \cline{1-6}
      \bottomrule
    \end{tabular}
    
    \note[item]{Dataset sizes: 100, 10, 1, 0.1 percent}
    \note[item]{The three architecture each with three pre-trained weights}
    \note[item]{Again this shows that using pretrained weights is important}
  \end{table}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{Reduction of Data Required}
  \begin{table}
    \centering
    \caption{Coefficients of the OLS showing the influence of the hyperparameters on the Evaluation Jaccard Index.}
    \color{black}
    \begin{tabular}{lrrrrrr}
      \toprule
                                   & Coef. & Std.Err. & t     & P>|t|         & [0.025 & 0.975] \\
      \midrule
      Intercept                    & 0.16  & 0.02     & 7.56  & \textbf{0.00} & 0.12   & 0.20   \\
      weights[T.imagenet]          & 0.23  & 0.02     & 11.66 & \textbf{0.00} & 0.19   & 0.27   \\
      weights[T.vae-b10]           & 0.01  & 0.02     & 0.67  & 0.50          & -0.03  & 0.05   \\
      architecture[T.fpn]          & 0.06  & 0.02     & 3.19  & \textbf{0.00} & 0.02   & 0.10   \\
      architecture[T.unet]         & 0.04  & 0.02     & 2.05  & \textbf{0.05} & 0.00   & 0.08   \\
      $\log_{10}(\text{fraction})$ & 0.02  & 0.01     & 2.71  & \textbf{0.01} & 0.00   & 0.03   \\
      \bottomrule
    \end{tabular}
    
    \note[item]{Same analysis as previous}
    \note[item]{Increasing the dataset size is useful, however only marginally}
    \note[item]{Most important factors remain using good pre-trained weights, architecture and dataset size are way less important.}
  \end{table}
  
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{Model Characteristics}
  \vspace*{-0.5cm}
  \begin{table}[ht]
    \centering
    % \caption{Characteristics of the various architectures with ResNet50 as backbone. Inference measurements were done on a NVIDIA GTX 1070 with an image shape of 128x128.}
    \color{black}
    \begin{tabular}{llrrr}
      \toprule
                                               & Batch Size & vaes    & unet            & fpn             \\
      \midrule
      Parameters (x$1e^6$)                     &            & 132.88  & 32.52           & \textbf{26.12}  \\
      \cline{1-5}
      Total MAC (x$1e^9$)                      &            & 7.85    & 2.72            & \textbf{1.95}   \\
      \cline{1-5}
      \multirow[t]{4}{*}{Inference Speed (ms)} & 1          & 19.13   & 8.52            & \textbf{7.95}   \\
                                               & 2          & 20.97   & 8.79            & \textbf{7.71}   \\
                                               & 8          & 42.35   & 16.74           & \textbf{15.47}  \\
                                               & 32         & 97.16   & 49.84           & \textbf{47.09}  \\
      \cline{1-5}
      \multirow[t]{4}{*}{Memory Usage (mb)}    & 1          & 1375.44 & 215.43          & \textbf{137.36} \\
                                               & 2          & 1381.01 & 221.59          & \textbf{141.69} \\
                                               & 8          & 1400.93 & 243.88          & \textbf{168.82} \\
                                               & 32         & 1881.55 & \textbf{436.95} & 439.22          \\
      \cline{1-5}
      \bottomrule
    \end{tabular}
  \end{table}
  
  \note[item]{NVIDIA GTX 1070}
  \note[item]{128x128 image}
  \note[item]{Resnet50 Backbone}
  \note[item]{Parameters}
  \note[item]{MAC}
  \note[item]{Inference Speed}
  \note[item]{Memory usage}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Experiments \& Results}
  \framesubtitle{Ablation Study -- Skip Connections}
  \vspace*{-1cm}
  \color{black}
  \begin{table}[ht]
    \centering
    \label{tab:skip_results}
    \begin{tabular}{llrrr}
                                                   & skip\_type & proj           & skip            & var     \\
                                                   & skip\_num  &                &                 &         \\
      \midrule
      \multirow[t]{5}{*}{Evaluation Jaccard Index} & 1          & 0.412          & \textbf{0.426}  & 0.410   \\
                                                   & 2          & \textbf{0.462} & 0.452           & 0.429   \\
                                                   & 3          & 0.474          & \textbf{0.493}  & 0.463   \\
                                                   & 4          & 0.456          & \textbf{0.494}  & 0.434   \\
                                                   & 5          & 0.440          & \textbf{0.490}  & 0.433   \\
      \cline{1-5}
      \multirow[t]{5}{*}{Parameters (x$1e^6$)}     & 1          & 67.160         & \textbf{29.410} & 104.911 \\
                                                   & 2          & 78.958         & \textbf{31.769} & 126.147 \\
                                                   & 3          & 81.907         & \textbf{32.359} & 131.456 \\
                                                   & 4          & 82.645         & \textbf{32.506} & 132.784 \\
                                                   & 5          & 82.700         & \textbf{32.525} & 132.876 \\
      \cline{1-5}
      \bottomrule
    \end{tabular}
  \end{table}
  
  \note[item]{Amount of connections, small effect, not significant (just)}
  \note[item]{Skip slightly better, small effect, not significant (just)}
  \note[item]{Bigger sample is required to tell if it is significant}
  \note[item]{more skip slower}
\end{frame}

\begin{frame}
  \frametitle{Experiments \& Results}
  \framesubtitle{Ablation Study -- Skip Connections}
  \color{black}
  \begin{table}[ht]
    \centering
    \caption{VAE and VAES results of various backbones.}
    \label{tab:vae-backbones-results}
    \begin{tabular}{lrrrrr}
      \toprule
                   & Params ($1e^6$) & MAC ($1e^9$)  & L1Loss        & $\frac{\text{KL-Divergence}}{\text{Pixel}}$ & Jaccard Index \\
      Backbone     &                 &               &               &                                                             \\
      \midrule
      EfficientNet & 12.09           & 0.91          & 5.99          & \textbf{2.88}                               & \textbf{0.47} \\
      MobileNetV2  & \textbf{6.05}   & \textbf{0.77} & 6.74          & 3.11                                        & 0.42          \\
      ResNet18     & 20.67           & 1.96          & \textbf{5.94} & 4.07                                        & 0.42          \\
      ResNet50     & 132.87          & 7.80          & 6.29          & 4.27                                        & 0.46          \\
      \bottomrule
    \end{tabular}
  \end{table}
  
  \note{Different backbones}
  \note{Choose ResNet50 as it is a safe, however this shows that smaller backbones might be better. Not large enough sample to determine statistically.}
  
\end{frame}

\begin{frame}
  \frametitle{Discussion}
  \begin{itemize}
    \pause
    \item Smaller Backbone
          \pause
    \item One small Dataset \pause
    \item Pretrained weights \pause
  \end{itemize}
  
  \note[item]{A baseline architecture had to be chosen for the comparative study, in hindsight this might have been able to be a smaller model.}
  \note[item]{The study was limited to the CoCo dataset, which although containing over 100k images is nothing compared to ImageNet (1M) or JFT-300M (300M)}
  \note[item]{Pretrained are beneficial, however the once trained using VAE with the CoCo dataset do not seem to effect the result in any way. A larger pretraining dataset might help.}
  \note[item]{Using pretrained weights does not reduce the amount of task specific data that is required for a segmentation.}
\end{frame}

\begin{frame}
  \frametitle{Conclusion}
  \begin{enumerate}
    \pause
    \item The VAE can be adapted.
          \pause
    \item Pretrained weights from VAE on CoCo do not effect the performance of sementic segmentation.
          \pause
    \item Pretraining does not reduce the amount of task specific labelled data required.
  \end{enumerate}
  
  \note[item]{We showed that we can adapt a VAE to segmentation, showing that it can be solved in a generative manner.}
  
\end{frame}

\end{document}
