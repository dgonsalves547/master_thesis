\chapter{Literature Review}
Currently state of the art models in machine learning often perform best when using a pretrained backbone which is often trained on well known public datasets, such as ImageNet \cite{deng2009imagenet} for image related tasks or Common Crawl \cite{commoncrawl} for text-based tasks. These pretrained backbones or foundation models, are then used for fine-tuning on specific tasks.

The main benefit of using pretrained networks are two-fold: they typically improve the convergence speed of the model and reduce the amount of task-specific data required \cite{donahue2014decaf,zeiler2014visualizing}. Furthermore, many the state of the art models in various (image) related task, such as object detection\cite{liu2016ssd,redmon2016you} and semantic segmentation \cite{orsic2019defense,girshick2014rich} benefit from pretrained backbones.

A drawback of these generic datasets is that, in many real-world robotic applications, the actual data distribution encountered is a tiny subset of those seen during training. E.g.\ a robot tasked with navigating in an indoor warehouse is unlikely to encounter an elephant. Thus the parts of the pretrained model that are able to detect elephants, or other animals, will not be used and take up precious computational resources on the device. Thus by pretraining a model on the actual data distribution it will encounter, the model could be made smaller or achieve higher accuracy.

However, the downside is that current pretraining tasks usually require the data to be labeled, which is an expensive endevor. Therefor, having a unsupervised pretraining method could be beneficial. A Variational Auto-Encoder (VAE) can be trained purely on images without requiring any manual labeling.

\section{VAE}
The VAE was first proposed by Kingma et al. \cite{kingma2014autoencodingvariationalbayes}. The task of a VAE is to reconstruct an image based on a (smaller) latent variable, and thus results in a type of lossy compression. The objective function of an VAE is the evidence lower bound (ELBO), is shown in Eq.~\ref{eq:elbo}.
\begin{equation}
    \label{eq:elbo}
    \mathcal{L}_{elbo} = \mathbb{E}_{q_{\phi}(z|x)}[\log p(x|z)] - D_{KL}(q_{\phi}(z|x) || p(z))
\end{equation}
where $\mathcal{L}$ is the ELBO, $x$ represents the input data, $z$ is the latent variable, $p(x|z)$ is the likelihood of observing the input data given the latent variable, $q_\phi(z|x)$ is the variational distribution or approximate posterior, and $D_{KL}(q_{\phi}(z|x) || p(z))$ is the Kullback-Leibler (KL) divergence between the variational distribution and the prior. By choosing a prior for which the KL divergence can be integrated analytically, we only need to approximate the reconstruction error $\mathbb{E}_{q_{\phi}(z|x)}[\log p(x|z)]$ using sampling. Empiracally, Kingma et al. found that when the batch size is large enough, a single sample per image is enough. This allows us to efficiently optimize the VAE's parameters and reduce the computational complexity of training. The last hurdle to approximate the distributions $q_{\phi}(z | x)$ and $p_{\theta}(x | z)$ with neural networks is that distributions are not differentiable. However, this can be sidestepped using the reparameterization trick. A differentiable tranformation $f_\phi(x, \epsilon)$. $\epsilon$ is drawn from a random distribution $p(\epsilon)$. An example for the gaussian is shown in Eq.~\ref{eq:reparameterization-trick}.
\begin{equation}
    \begin{split}
        \mu, \sigma & = f_phi{x}                  \\
        \epsilon    & \sim \mathcal{N}(0, 1)      \\
        z           & = \mu + \sigma^2 * \epsilon
    \end{split}
    \label{eq:reparameterization-trick}
\end{equation}
This can easilly be extended to a broad range of distributions.

However, there exist a few problems with VAEs\cite{tomczak2021deep}. Such as \emph{blurry samples}, see Appendix~\ref{appendix:reconstruction_samples}\todo{Add reconstruction samples in the appendix}, \emph{posterior collapse}\cite{DBLP:journals/corr/BowmanVVDJB15} and the fact that the latent space is not necesarrily 'disentangeld'~\cite{higgins2017betavae}. A latent space that is properly disentangeld will have ideally one latent unit being responsible for one aspect of the generative process. Such that changing that unit, results in a single (semantic) change in the generated sample. In the case of face generation this could for example be the shape of the nose, or the color of the skin. Given a properly disentangeld space makes it easier to finetune models on subsequent tasks\cite{bengio2014representationlearningreviewnew}, and thus is of paramount importance to us. The $\beta$-VAE, proposed by Higgins et al. \cite{higgins2017betavae}, multiplies the KL divergence with a hyperparameter $\beta$, resulting in Eq.~\ref{eq:beta-elbo}. If this value is set to 1, it results in a standard VAE. In their paper, they show that for $\beta$ values greater then one, the learnt latent representation are more disentangeld. Note, that there is no additional information required about the data for this to be learned. By increasing the $\beta$, the capacity of the latent space is reduced. Meaning, that less information can be passed due to the stronger regularization force of the KL-divergence.
\begin{equation}
    \mathcal{L}_{\beta-elbo} = \mathbb{E}_{q_{\phi}(z|x)}[\log p(x|z)] - \beta * D_{KL}(q_{\phi}(z|x) || p(z))
    \label{eq:beta-elbo}
\end{equation}
However, when the $\beta$ is to large, it will result in a posterior collapse. As the model is strongly incentivized to ensure that the posterior is equal to the prior.

There exists many more extensions to the classical VAE, most of which can be divided into one of the following four categories. \emph{Better encoders}, such as flow-based models \cite{Berg2018SylvesterNF,tomczak2017improving,rezende2015variational}. \emph{Better priors}, such as VampPrior \cite{tomczak2018vae} in which the prior is a mixture of Gaussians, with additional pseudo-inputs learned from the training data to prevent overfitting. \emph{Better decoders}, such as tranformer based decoders \cite{Henderson2022AVA,9054554}. And lastly, Hierarchical-VAEs (HVAE), of which the most noteable are the Ladder-VAE\cite{NIPS2016_6ae07dcb}, the BIVA \cite{maaloe2019biva} and NVAE \cite{vahdat2020nvae}. These models extract a latent space from multiple levels of the encoder. During the generation phase, these latents variables are then generated based on the decoder. Compared to classic-VAEs they are capable of generating far sharper images. However, they are more complicated to train, and often consist of more parameters. Thus resulting in a slower inference.

Another benefit of the VAE, is that it can be used for rudementary anomaly detection. There are two main methods with which this can be done. Either from the posterior output $p_{\phi}(z|x)$, by detecting unusual latent variables \cite{marimont2020anomalydetectionlatentspace,angiulli2020improving,angiulli2023latent}. Or calculating the reconstruction error from the given input \cite{an2015variational, zhou2020unsupervisedanomalylocalizationusing, gouda2022unsupervised}. The main benefit of the first method is that the decoder is not required for anomaly detection, resulting in a lower inference cost. In the case of robotics, it is important to detect unusual situation as it might result in unexpected behaviour which inturn could lead to dangerous situations.
A closely related use case is the ability to have an uncertainty estimation, by the use of bootstrapping \cite{chen2018use,kohl2018probabilistic}, for the output of the model. The additional uncertainty information could then be used by downstream systems to be even more robust. Furthermore, this information can be used for various active learning \cite{hino2020active} techniques. Which allow for more effective labeling strategies, thus reducing the amount of labeling required. \todo{Maybe move this to future work. Mainly the part about active learning as I could not find papers which utilize VAE-bootstrapping for this.}

\section{Semantic Slam}
Simultaneous localication and mapping (SLAM)\cite{chatila1985position} is a powerful method used for the automated navigation of robotic vehicles. It is capable of simultaneously creating a map of an unknown environment and navigating that same environment. A vital part for autonomous robotic vehicles. With the rise of deep learning methods in computer vision, many visual based SLAM algorithms have been created \cite{taketomi2017visual}. More recent algorithms use semantic segmentation networks \cite{yu2018ds}. It uses SegNet \cite{badri2017segnet}, to extract the semantic information from images. By improving the quality of this model, the complete system would become more stable.

\subsection{Semantic segmentation}
Semantic segmentation is a computer vision technique that involves assigning a label or category to each pixel in an image. This means that, rather than just detecting objects, the method also identifies what those objects are (e.g.\, chair, person, road). The goal of semantic segmentation is to produce a dense prediction map that classifies every pixel in the image into one of the predefined categories.

One of the first more successful deeplearning methods for semantic segmentation was the Fully Convolutional Network (FCN) proposed by Long et al. \cite{long2015fully}. Which was slightly improved by \cite{ronneberger2015u}, and named U-net. Referencing the shape of the model. The architecture of the model can be seen in Figure~\ref{fig:unet-architecture}. Due to the structure, the output can take both fine-grained information, which is useful to have good localization and global information, which improves the classification. This method is robust and reliable, making it easy to train and use in real-time applications.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/unet-architecture.png}
    \caption{U-Net Architecture \cite{ronneberger2015u}}
    \label{fig:unet-architecture}
\end{figure}

Feature Pyramid Network (FPN) \cite{lin2017feature} shares part of it structure with the UNet. Except it explicitly combines the features of each upscaling layer to predict the final semantic segmentation mask as visualized in Figure~\ref{fig:fpn-architecture}. The main difference between FPN and U-Net is that FPN takes more time to process images. It does produce better results for smaller and medium-sized objects.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fpn-architecture.png}
    \caption{Feature Pyramid Network Architecture \cite{lin2017feature}}
    \label{fig:fpn-architecture}
\end{figure}

Although the above models are not state of the art anymore, they remain easy to train and do not require exotic tricks to make them work. Moreover, they have good inference speed and are architecturely similar to our proposed method. The primary issue with these models is their limited field of view, resulting in dificulty in relating parts of the image that are far apart. This can partially be combated by making use of dilated convolutional layers, such as proposed by Gao \cite{gao2023rethinking}. Or by the use of vision tranformers \cite{dosovitskiy2021image}, which can relate pixels that are far apart in the input image. These vision tranformers have been used successfully in the segmentation task \cite{xie2021segformer,chen2022vision}. However, due to the computational complexity in vision tranformers they are not yet suitable for real-time inference on mobile robotics. $TODO$ Reference to 'no new-net', of why UNet is good enough for our use-case.

Another impressive example is the Segment Anything Model (SAM) \cite{kirillov2023segment}. It is a huge, and slow, network capable of, as the name implies, anything based on a prompt. Although not relevant for comparison, it can be used to aid the labour intensive task of labeling new data.
