\chapter{Method}\label{chapter:first_real_chapter}
From a probabilistic point of view, semantic segmentation can be modeled as $p_\theta(x,y)$, where $x$ is a given (rgb)-image, and $y$ is the semantic label. However, in practice there also exist $z$, the physical object of which $x$ is a view. Thus we propose the probabilistic model to be $p_\theta(x,y,z)$. Which can be rewritten as: $p_\theta(x,y,z)$ = $p_\theta(y|x,z) p_\theta(x|z) p_\theta(z)$. With $p_\theta(z)$ being the generative process of the object. $p_\theta(x|z)$ the generative process of taking the picture of the object and, finally $p_\theta(y|x,z)$ being the process of placing the semantic meaning of $z$ in the viewport of $x$. Notice that the model $p_\theta(x|z) p_\theta(z)$ is equal to the VAE. Thus we prose to first learn the generative model $p_\theta(x,z) = p_\theta(x|z) p_\theta(z)$, by learning approximations of $p_\theta(x|z)$ and $p_\theta(z)$, using a VAE. After which we can use that model to find $p_\theta(y|x,z)$.

\section{Model}
We start by rewritting the log likelihood of $p(x, y, z)$ as shown in Eq.~\ref{eq:label_marginal}. Notice, that the second right hand term, is the log likelihood of $p(x)$. Using the Jensen's inequality, it can be replaced with the $\mathcal{L}_elbo$. Thus resulting in the $\mathcal{L}_{label-elbo}$, shown in Eq.~\ref{eq:label_elbo}

\begin{subequations}
    \begin{align}
        log p_\theta(x, y) & = \int_z log p_\theta(y | x, z) dz +  \int_z log p_\theta(x, z) + log p_\theta(z) dz             \label{eq:label_marginal} \\
                           & \geq log p_\theta(y | x) + \mathcal{L}_{elbo}                                                                              \\
                           & \approx \mathbb{E}_{z~q_{\phi}(z | x)}[\log p(y|x, z)] + \mathcal{L}_{elbo}                                                \\
                           & = \mathcal{L}_{label-elbo} \label{eq:label_elbo}
    \end{align}
\end{subequations}

Hence, we want to estimate the following three distributions we want to estimate using an neural network.
\begin{itemize}
    \item $q_\phi(z|x)$ (i.e. image encoder)
    \item $p_\theta(x|z)$ (i.e. image decoder)
    \item $p_\theta(y|x, z)$ (i.e. label decoder)
\end{itemize}
We will refer to the image encoder and label decoder together as VAE-Segmentation (VAES). A schematic of the model can be seen in Figure~\ref{fig:schematic-vaes}.

\begin{figure}[h]
    \centering
    \subfloat[Classic VAE\label{fig:schematic-vae}]{\includegraphics[width=0.45\textwidth]{figures/vae.png}}\hphantom{space}
    \subfloat[Our method (VAES)\label{fig:schematic-vaes}]{\includegraphics[width=0.45\textwidth]{figures/vaes.png}}
    \caption{A schematic of the proposed VAES.}
\end{figure}

\subsection{Image Encoder}
The image encoder approximates the latent distribution, $p(z)$. To ensure we can train the encoder using gradient descent, we need to make sure it is fully differentiable. Thus we make use of the reparameterization trick. Using a deterministic mapping $g_\phi(\epsilon, z')$, where $\epsilon$ is an independent random variable and $g_\phi(x)$ is our (deterministic) neural network. Kingma et al. show that using this reparameterization trick, a wide range of distributions can be learned. In our case we use a Gaussian latent space. The reparameterization then becomse $z = \mu + \sigma \epsilon$, where $\mu$ and $\sigma$ are the output of our image encoder network.

\subsection{Image Decoder}
The image decoder approximates the conditional Gaussian distribution $p_\theta(x|z)$. It is used during the pretraining step to prime the image encoder with 'good' initial weights, using the traditional ($\beta$)-VAE training. The hypothesis is, that given that $x$ can be reconstructed from the learned $q_\phi(z|x)$, the learned latent space contains useful features to approximate $p_\theta(y|z)$. In Eq.~\ref{eq:beta-elbo} $log p(x|z)$ can be calculated using Eq.~\ref{eq:log_p_x_z} using a parameterized neural network.

\begin{equation}
    \begin{split}
        log p(x|z)              & = log \mathcal{N}(x; \mu, \sigma^2I) \label{eq:log_p_x_z} \\
        \text{where}~\mu,\sigma & =NN_\theta(z)
    \end{split}
\end{equation}

\subsection{Label Decoder}
The label decoder is similar to the image decoder, except that it approximates a multivariate Bernoulli distribution, instead of an Gaussian. Thus the only difference is the calculation of $log p(y|z,x)$, which is shown in Eq.~\ref{eq:log_p_y_z}.

\begin{subequations}
    \begin{align}
        log p(y|z, x)  & = \sum_{i=0}^n y_i log h_i + (1 - y_i)log(1-h_i) \label{eq:log_p_y_z} \\
        \text{where}~h & = softmax(NN_\theta(z))
    \end{align}
\end{subequations}

To improve the sharpness of the labels, additional latent variables are added from higher up the encoder, similar to HVAE. This results in the final loss Eq.~\ref{eq:beta-hvae-label-elbo}.

\begin{equation}
    \label{eq:beta-hvae-label-elbo}
    \mathcal{L}_{\beta-label} = \mathbb{E}_{q_{\phi}(z|x)}[\log p(y|z)] - \beta \sum_{i=0}^n(D_{KL}(q_{\phi}(z_i|x) || p(z_i)))
\end{equation}

\begin{figure}[h]
    \begin{minipage}{0.9\textwidth}
        \includegraphics[width=1\textwidth]{figures/model_data_flow.png}
        \caption{The data flow of the various submodels. NOTE: I still want to make this prettier, making it more similar to \ref{fig:hvae-example}}
        \label{fig:seg-vae-schematic}
    \end{minipage}
\end{figure}

\begin{figure}
    \begin{minipage}{0.9\textwidth}
        \includegraphics[width=1\textwidth]{figures/h_vae_structure.png}
        \caption{Example of the H-VAE structure. From \cite{kohl2018probabilistic}.}
        \label{fig:hvae-example}
    \end{minipage}
\end{figure}

\section{Training procedure}
The training consist of two stages, the pre-training phase, and the fine-tuning stage. 
\paragraph*{Pre-training phase} During the pre-training phase, the image encoder, $q_\phi(z|x)$, and image decoder, $p_\theta(x|z)$, are trained. The model is optimized to minimize Eq.~\ref{eq:beta-elbo} using stochastic gradient descent with minibatches. Note, that during pre-training no labels are required.

\paragraph*{Fine-turning phase} During the fine-tuning phase, the learned image encoder, $q_\phi(z|x)$ is reused. However, the image decoder, $p_\theta(x|z)$, is replaced with a new label decoder, $p_\xi(x|z)$. The encoder can either be kept frozen or jointly optimized with the decoder. During fine-tuning Eq.~\ref{eq:label_elbo} is maximized.

In the case that the image encoder is kept frozen, the dataset can be encoded once into the latent space. This latent space can then be sampled, reducing the computational and memory required during training.

\section{Evaluation}
\subsection{Inference}
Due to the variational architecture of the model, the inference can be done either deterministic, by taking the mean of each latent vector. Or variational, by sampling each latent vector. During all evaluation stages, the mode of the latent space is used.

\subsection{Metrics}
Within classification the most well-known and widely used metrics are precision (\ref{eq:precision}), recall (\ref{eq:recall}), and F1-score (\ref{eq:f1})\cite{rijsbergen1979information}. Of which the latter is a combination of precision and recall. Within the field of image segmentation, the F1-score is also sometimes refered to as the Recognition Quality (RQ). TP, FP, and FN, refer to the True Positive, False Positive and False Negative predictions per pixel. As our model always gives a prediction for each pixel, a False Positive in one class, always coincides with a False Negative in another class.Another common metric is the Jaccard Index (Eq.~\ref{eq:jaccard}). It is monotonically similar to the F1, however it weights incorrect predictions.

\begin{subequations}
    \begin{align}
        \text{Precision}     & = \frac{TP}{TP + FP} \label{eq:precision}                                                                                                     \\
        \text{Recall}        & = \frac{TP}{TP + FN} \label{eq:recall}                                                                                                        \\
        F1 = \text{RQ}       & = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\label{eq:f1}  = \frac{TP}{\frac{1}{2} (FN + FP) + TP} \\
        \text{Jaccard Index} & = \frac{TP}{FN + FP + TP} \label{eq:jaccard}                                                                                                  \\
    \end{align}
\end{subequations}
