\chapter{Background}
Currently, state of the art models in machine learning often perform best when using a pretrained backbone. These backbones are trained on well known public datasets, such as ImageNet \cite{deng2009imagenet} for image related tasks or Common Crawl \cite{commoncrawl} for text-based tasks. These pretrained backbones or foundation models, are then used for fine-tuning on specific tasks. The benefits, and risks, are described in great detail by Bommasani et al. \cite{DBLP:journals/corr/abs-2108-07258}. We will highlight a few of them here.

The main benefit of using pretrained networks are two-fold: they typically improve the convergence speed of the model and reduce the amount of task-specific data required \cite{donahue2014decaf,zeiler2014visualizing}. Furthermore, many the state of the art models in various (image) related task, such as object detection\cite{liu2016ssd,redmon2016you} and semantic segmentation \cite{orsic2019defense,girshick2014rich} benefit from pretrained backbones.

A drawback of these generic datasets is that, in many real-world robotic applications, the actual data distribution encountered is a tiny subset of those seen during training. Thus the question arises; does a robot tasked with navigating an indoor warehouse benefit from knowing how to detect an elephant, or other (big) animals? Especially in the case of mobile robotics where the computational resources are scarce. Thus by pretraining on the actual dataset, could we avoid learning these useless parts possibly increase the accuracy on the task at hand? However, these task specific datasets are often non-existend, and although collecting task relevant data can often easilly be done, e.g. collecting videos of a warehouse by manually driving a robot around for a day. It remains expensive to convert this raw data into a useful dataset due to the labour required to \emph{properly} label it. In their systematic review of imagenet, Mishkin et al.\cite{MISHKIN201711}, show that a smaller high quality dataset results in better performance compared to a larger low quality dataset. Therefor, it would be ideal if we could learn most of the important features from the raw data, i.e. unsupervised learning.

\section{Variational Auto-Encoder}
A Variational Auto-Encoder (VAE) is a model that can be trained purely on images without requiring any manual labeling. The concept of the VAE was independently proposed by Kingma et al. \cite{kingma2014autoencodingvariationalbayes} and Rezende et al.\cite{rezende2014stochastic}. The general idea behind the VAE is that there is a complex parameterizable distribution $p_\theta(x)$ of interest. Either to deepen the understanding of the process by which it is generated, or to allow us to mimic the natural data. It is assumed that the distribution is the result of a process involving some prior distribution $p_\theta(z)$. First a value $z$ is generated by $p_\theta(z)$, after which $x$ is generated from the conditional distribution $p_\theta(x | z)$. Which together can be modeled as $p_\theta(x, z) = p_\theta(x|z)p_\theta(z) = p_\theta(z|x)p_\theta(x)$. The VAE proposes to approximate the conditional distribution $p_\theta(z|x)$, using an \emph{encoder} model $q_\phi(z|x)$. The marginal log likelihood can be described by Eq.~\ref{eq:marginal_likelihood} and as the Kullback-Leiler (KL) Divergence is $\geq 0$, the remaining term is the lower bound of the marginal log likelihood. The remaining term can be further rewritten into Eq.~\ref{eq:elbo}.
\begin{subequations}
    \begin{align}
        log p_\theta(x)    & = \mathbb{E}_{q_{\phi}(z|x)}[-log q_\phi(z|x) + \log p(x,z)] + D_{KL}(q_{\phi}(z|x) || p_\theta(z|x))\label{eq:marginal_likelihood} \\
        log p_\theta(x)    & \geq \mathbb{E}_{q_{\phi}(z|x)}[-log q_\phi(z|x) + \log p(x,z)] = \mathcal{L}_{elbo}                                                \\
        \mathcal{L}_{elbo} & = \mathbb{E}_{q_{\phi}(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_{\phi}(z|x) || p_\theta(z))\label{eq:elbo}
    \end{align}
\end{subequations}
This lower bound can then be optimized w.r.t. to our parameters using gradient optimization. By choosing a prior $p_\theta(z)$ for which the KL divergence can be integrated analytically, we only need to approximate the reconstruction error $\mathbb{E}_{q_{\phi}(z|x)}[\log p(x|z)]$ using sampling. Empiracally, Kingma et al. found that when the batch size is large enough, a single sample per image is enough. This allows us to efficiently optimize the VAE's parameters and reduce the computational complexity of training. The last hurdle to approximate the distributions $q_{\phi}(z | x)$ and $p_{\theta}(x | z)$ with neural networks is that distributions are not differentiable. However, this can be sidestepped using the reparameterization trick. A differentiable tranformation $f_\phi(x, \epsilon)$. $\epsilon$ is drawn from a random distribution $p(\epsilon)$. An example for the gaussian is shown in Eq.~\ref{eq:reparameterization-trick}.
\begin{equation}
    \begin{split}
        \mu, \sigma & = f_phi{x}                  \\
        \epsilon    & \sim \mathcal{N}(0, 1)      \\
        z           & = \mu + \sigma^2 * \epsilon
    \end{split}
    \label{eq:reparameterization-trick}
\end{equation}
This can easilly be extended to a broad range of distributions.

However, there exist a few problems with VAEs\cite{tomczak2021deep}. Such as \emph{blurry samples}, see Appendix~\ref{appendix:reconstruction_samples}\todo{Add reconstruction samples in the appendix}, \emph{posterior collapse}\cite{DBLP:journals/corr/BowmanVVDJB15} and the fact that the latent space is not necesarrily 'disentangeld'~\cite{higgins2017betavae}. A latent space that is properly disentangeld will have ideally one latent unit being responsible for one aspect of the generative process. Such that changing that unit, results in a single (semantic) change in the generated sample. In the case of face generation this could for example be the shape of the nose, or the color of the skin. Given a properly disentangeld space makes it easier to finetune models on subsequent tasks\cite{bengio2014representationlearningreviewnew}, and thus is of paramount importance to us. The $\beta$-VAE, proposed by Higgins et al. \cite{higgins2017betavae}, multiplies the KL divergence with a hyperparameter $\beta$, resulting in Eq.~\ref{eq:beta-elbo}. If this value is set to 1, it results in a standard VAE. In their paper, they show that for $\beta$ values greater then one, the learnt latent representation are more disentangeld. Note, that there is no additional information required about the data for this to be learned. By increasing the $\beta$, the capacity of the latent space is reduced. Meaning, that less information can be passed due to the stronger regularization force of the KL-divergence.
\begin{equation}
    \mathcal{L}_{\beta-elbo} = \mathbb{E}_{q_{\phi}(z|x)}[\log p(x|z)] - \beta * D_{KL}(q_{\phi}(z|x) || p(z))
    \label{eq:beta-elbo}
\end{equation}
However, when the $\beta$ is to large, it will result in a posterior collapse. As the model is strongly incentivized to ensure that the posterior is equal to the prior.

There exists many more extensions to the classical VAE, most of which can be divided into one of the following four categories. \emph{Better encoders}, such as flow-based models \cite{Berg2018SylvesterNF,tomczak2017improving,rezende2015variational}. \emph{Better priors}, such as VampPrior \cite{tomczak2018vae} in which the prior is a mixture of Gaussians, with additional pseudo-inputs learned from the training data to prevent overfitting. \emph{Better decoders}, such as tranformer based decoders \cite{Henderson2022AVA,9054554}. And lastly, Hierarchical-VAEs (HVAE), of which the most noteable are the Ladder-VAE\cite{NIPS2016_6ae07dcb}, the BIVA \cite{maaloe2019biva} and NVAE \cite{vahdat2020nvae}. These models extract a latent space from multiple levels of the encoder. During the generation phase, these latents variables are then generated based on the decoder. Compared to classic-VAEs they are capable of generating far sharper images. However, they are more complicated to train, and often consist of more parameters. Thus resulting in a slower inference.

\todo{Maybe move this to future work. Mainly the part about active learning as I could not find papers which specifically use uncertainty measurements from VAE/bootstrapping for this.}
Another benefit of the VAE, is that it can be used for rudementary anomaly detection. There are two main methods with which this can be done. Either from the approximate posterior output $q_{\phi}(z|x)$, by detecting unusual latent variables \cite{marimont2020anomalydetectionlatentspace,angiulli2020improving,angiulli2023latent}. Or calculating the reconstruction error from the given input \cite{an2015variational, zhou2020unsupervisedanomalylocalizationusing, gouda2022unsupervised}. The main benefit of the first method is that the decoder is not required for anomaly detection, resulting in a lower inference cost. In the case of robotics, it is important to detect unusual situation as it might result in unexpected behaviour which inturn could lead to dangerous situations.
A closely related use case is the ability to have an uncertainty estimation, by the use of bootstrapping \cite{chen2018use,kohl2018probabilistic}, for the output of the model. The additional uncertainty information could then be used by downstream systems to be even more robust. Furthermore, this information can be used for various active learning \cite{hino2020active} techniques. Which allow for more effective labeling strategies, thus reducing the amount of labeling required.

\section{Semantic Slam}
Simultaneous localication and mapping (SLAM)\cite{chatila1985position} is a powerful method used for the automated navigation of robotic vehicles. It is capable of simultaneously creating a map of an unknown environment and navigating that same environment. A vital part for autonomous robotic vehicles. With the rise of deep learning methods in computer vision, many visual based SLAM algorithms have been created \cite{taketomi2017visual}. More recent algorithms use semantic segmentation networks \cite{yu2018ds}. It uses SegNet \cite{badri2017segnet}, to extract the semantic information from images. By improving the quality of this model, the complete system would become more stable.

\subsection{Semantic segmentation}
Semantic segmentation is a computer vision technique that involves assigning a label or category to each pixel in an image. This means that, rather than just detecting objects, the method also identifies what those objects are (e.g.\, chair, person, road). The goal of semantic segmentation is to produce a dense prediction map that classifies every pixel in the image into one of the predefined categories.

One of the first more successful deeplearning methods for semantic segmentation was the Fully Convolutional Network (FCN) proposed by Long et al. \cite{long2015fully}. Which was slightly improved by \cite{ronneberger2015u}, and named U-net. Referencing the shape of the model. The architecture of the model can be seen in Figure~\ref{fig:unet-architecture}. Due to the structure, the output can take both fine-grained information, which is useful to have good localization and global information, which improves the classification. This method is robust and reliable, making it easy to train and use in real-time applications.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/unet-architecture.png}
    \caption{U-Net Architecture \cite{ronneberger2015u}}
    \label{fig:unet-architecture}
\end{figure}

Feature Pyramid Network (FPN) \cite{lin2017feature} shares part of it structure with the UNet. Except it explicitly combines the features of each upscaling layer to predict the final semantic segmentation mask as visualized in Figure~\ref{fig:fpn-architecture}. The main difference between FPN and U-Net is that FPN takes more time to process images. It does produce better results for smaller and medium-sized objects.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fpn-architecture.png}
    \caption{Feature Pyramid Network Architecture \cite{lin2017feature}}
    \label{fig:fpn-architecture}
\end{figure}

These models are very adaptable as their various parts, such as encoders, can easilly be replaced. Thus they can be modified to adapt to the limitations of mobile robotics, by employing lightweight and fast encoders. Moreover, during the BRaTS2018 \cite{menze2014multimodal} competition, it has been shown to be very competitive achieving second place\cite{DBLP:journals/corr/abs-1809-10483}. Next to that, they also both employ an encoder-decoder structure, which is similar to the VAE. Thus resulting in a more representative comparison.

\paragraph{}
\todo{Rewrite + move this to future work}
Although the above models are not state of the art anymore, they remain easy to train and do not require exotic tricks to make them work. Moreover, they have good inference speed and are architecturely similar to our proposed method. The primary issue with these models is their limited field of view, resulting in dificulty in relating parts of the image that are far apart. This can partially be combated by making use of dilated convolutional layers, such as proposed by Gao \cite{gao2023rethinking}. Or by the use of vision tranformers \cite{dosovitskiy2021image}, which can relate pixels that are far apart in the input image. These vision tranformers have been used successfully in the segmentation task \cite{xie2021segformer,chen2022vision}. However, due to the computational complexity in vision tranformers they are not yet suitable for real-time inference on mobile robotics. $TODO$ Reference to 'no new-net', of why UNet is good enough for our use-case.
Another impressive example is the Segment Anything Model (SAM) \cite{kirillov2023segment}. It is a huge, and slow, network capable of, as the name implies, anything based on a prompt. Although not relevant for comparison, it can be used to aid the labour intensive task of labeling new data.
