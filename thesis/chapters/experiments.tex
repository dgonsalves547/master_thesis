\chapter{Experimental Setup}\label{chapter:second_real_chapter}

\subsection*{In distribution}
We will take a fully labeled dataset $\{\mathcal{X}, \mathcal{Y} \} \in \mathcal{D}$. First the "classic" H-VAE is trained, following Efficient-VDVAE \cite{hazami2022efficientvdvae}, on the the full set $\mathcal{X}$. After convergence, the pretrained $p_\theta$ is used to finetune $q_\xi$, on a (sub)set of $\mathcal{D}$.
Subsets of various sizes will be tested. Definitely  10, 50 and 100 percent of the (training) dataset will be tested, and if time allows more sizes will be tried.

\subsection*{Out of distribution}
A good foundation model should be able to be trained on a generic dataset, to be subsequently finetuned on a specific dataset. To understand if this is also the case for our method, we will first train the H-VAE on one dataset. This model will then be finetuned using a different dataset.

\subsection*{Novel Objects}
To understand the ability of the model to generalize over novel object, first we will remove all images that contain segmentations of a specific class. The foundation and segmentation model will be trained on the partial dataset first. During testing we expect to have a high uncertainty for the locations with the novel objects present.

\subsection*{General Metrics}
For all of the above experiments the following metrics will be tracked to get insight in the quality of the model.

\begin{table}[]
    \begin{tabular}{l|l|l}
        Metric                      & Formula                                   & Reason                                     \\
        \hline
        Precision                   & $\frac{TP}{TP + FP}$                      & Standard practice                          \\
        Recall                      & $\frac{TP}{TP + FN}$                      & Standard practice                          \\
        Segmentation quality        & $\frac{\|TP\|}{\|FP\| + \|TP\| + \|FN\|}$ & Quantifly compare accuracy of the model    \\
        Expected Callibration Error &                                           & Check uncertainty prediction is reliable.
    \end{tabular}
\end{table}

\footnote[1]{ECE is taken from \cite{guo2017calibration}}
