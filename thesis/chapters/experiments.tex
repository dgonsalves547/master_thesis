\chapter{Experimental Setup}\label{chapter:second_real_chapter}

\section{Dataset}
\paragraph{COCO.} COCO \cite{lin2015microsoftcococommonobjects} is a publicly available dataset and has a multitude of labels. The semantic mask are extrated from the 2017 Panoptic annotations. This set contains more then 100k images that are densly annotated with both the semantic class and instance. There are a total of 133 different classes which belong to 27 'supercategories'. To reduce the complexity and speed up convergence, only the supercategories are used.

\paragraph{CamVid}




\section{Ablation Study}

\subsection{Backbone}
Trying various backbones for VAE

\subsection{Beta}
Trying various Beta values for the loss, higher will likely result in a worse VAE model. However it may lead to a better VAsegm model.

\subsection{Freezing}
Trying to freeze the encoder part. An added benefit to this is that we still have our original $P(z | x)$ model, which can then be used to potentially detect distribution drift. Furthermore, it makes the switch to an multimodal VAE, such as proposed in \cite{vasco2020mhvae}, possible.

\subsection{Skip Connection}
To understand the importance of the skip connections which are added after the 

\subsection{Probabilistic Inference}
Another potential benefit of the VAEseg is the probabilistic nature of the model. Using bootstrapping, an uncertainty measure can be created. To understand if this certainty measure is a useful predictor of the actual certainty of the output. The ECE is reported for various sizes of bootstrapping.

\section{Understanding potential benefits of VAE}
\subsection*{ImageNet Bias}
\#TODO provide some examples of the (racial) bias problems of Imagenet. Might be able to generalize it to human made labels, hence benefit of VAE.

As previous research already has shown (TODO link research), models that use a pretrained backbone which is trained on the ImageNet dataset, contain various biases. As our backbone is not trained on ImageNet a small qualitative research will be done on the 2 models. To detect wheter the bias differs between the VASegm and an UNet-segmentation model. This is done by training two models, one using a frozen encoder which is trained on ImageNet, and one that is trained using the VAE. Based on some initial datavisualization it was suggested that ImageNet contained some racial biases in which it depicted persons of color more often as animals in comparison to the wrongly predicted caucasians people.

\subsection{}

\subsection*{Comparative study with discriminative methods}
The following semantic segmentation models will be tested
UNet (\cite{ronneberger2015u}, no pretraining)
UNet (With pretrained backbone (e.g. resnet))
VAE (No pretraining, various backbones can be tested)
VAE (Pretraining on images, various backbones can be tested)

One of the benefits of using VAE as basic architecture, is the possibility of pretraining using the reconstruction task. This could allow for a better initialization of the starting weigths, which in turn could lead to either faster or better convergence. However, in practice this was not the case. Looking at the first few convolutional layers might give insight in what the differenc tasks are prioritizing. Hence, a comparative study is done between the visualized filters.

To better understand the effects of the following reducing the amount of labeled data, the following questions should be answered.
\begin{itemize}
    \item What is the influence of a reduced amount of labeled data on the baseline network?
    \item What is the influence of a reduced amount of labeled data on the VAE network?
    \item What is the influence of a reduced amount of labeled data on the pretrained VAE network?
\end{itemize}
To do this, each model will be trained with a subset of the data and the performance degredation will be compared.


Furthermore, it might be nice to investigate if the dataset can contain non-overlapping data
\begin{itemize}
    \item 90 percent of training data for VAE training, other 10 percent for VASegm
    \item A subset of classes in training data to train VAE, then a few unseen classes for the VAsegm
\end{itemize}

Another possible experiment is, to view the uncertainty/variance of unseen classes. Take N samples from the latent space, to see if the variance of unknown objects have a higher variance then seen/known objects.

The three 'variables'
(not) seen in VAE training
(not) seen in VAEseg training
What is the variance for these objects in a test set.


Also try to make some of the skip connection not variational






% \subsection*{In distribution}
% We will take a fully labeled dataset $\{\mathcal{X}, \mathcal{Y} \} \in \mathcal{D}$. First the "classic" H-VAE is trained, following Efficient-VDVAE \cite{hazami2022efficientvdvae}, on the the full set $\mathcal{X}$. After convergence, the pretrained $p_\theta$ is used to finetune $q_\xi$, on a (sub)set of $\mathcal{D}$.
% Subsets of various sizes will be tested. Definitely  10, 50 and 100 percent of the (training) dataset will be tested, and if time allows more sizes will be tried.

\subsection*{Out of distribution}
A good foundation model should be able to be trained on a generic dataset, to be subsequently finetuned on a specific dataset. To understand if this is also the case for our method, we will first train the H-VAE on one dataset. This model will then be finetuned using a different dataset.

\subsection*{Novel Objects}
To understand the ability of the model to generalize over novel object, first we will remove all images that contain segmentations of a specific class. The foundation and segmentation model will be trained on the partial dataset first. During testing we expect to have a high uncertainty for the locations with the novel objects present.

\subsection*{General Metrics}
For all of the above experiments the following metrics will be tracked to get insight in the quality of the model.

\begin{table}[]
    \begin{tabular}{l|l|l}
        Metric                      & Formula                                   & Reason                                     \\
        \hline
        Precision                   & $\frac{TP}{TP + FP}$                      & Standard practice                          \\
        Recall                      & $\frac{TP}{TP + FN}$                      & Standard practice                          \\
        Segmentation quality        & $\frac{\|TP\|}{\|FP\| + \|TP\| + \|FN\|}$ & Quantifly compare accuracy of the model    \\
        Expected Callibration Error &                                           & Check uncertainty prediction is reliable.
    \end{tabular}
\end{table}

\footnote[1]{ECE is taken from \cite{guo2017calibration}}
