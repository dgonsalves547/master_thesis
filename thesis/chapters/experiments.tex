\chapter{Experimental Setup}\label{chapter:second_real_chapter}

\section{Implementation details}
\subsection{Dataset}
\paragraph{COCO~\cite{lin2015microsoftcococommonobjects}} is a publicly available dataset and has a multitude of labels. The semantic mask are extracted from the 2017 Panoptic annotations. This set contains more than 100k images that are densly annotated with both the semantic class and instance. There are a total of 133 different classes which belong to 27 `supercategories'. The four categories ``food'' and ``food-stuff'' and ``furniture'' and ``furniture-stuff'' were merged into ``food'' and ``furniture'' respectively, bringing the number of supercategories to 25. To speed up convergences, we will use these 25 classes. The distribution of the class labels can be seen in Figure \ref{fig:coco-class-distribution}. Some samples of the dataset can be seen in Figure \ref{fig:coco-samples}. More samples can be found in Appendix \ref{appendix:coco_samples}. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/datasets/coco/class_distribution.png}
    \caption{Class Distribution of CoCo}
    \label{fig:coco-class-distribution}
\end{figure}

\begin{figure}
    \centering
    \subfloat[Training sample 0]{%
        \includegraphics[width=0.5\textwidth]{figures/datasets/coco/samples/train/0.png}%
    }
    \subfloat[Training sample 1]{%
        \includegraphics[width=0.5\textwidth]{figures/datasets/coco/samples/train/1.png}%
    }\\
    \subfloat[Validation sample 0]{%
        \includegraphics[width=0.5\textwidth]{figures/datasets/coco/samples/val/0.png}%
    }
    \subfloat[Validation sample 1]{%
        \includegraphics[width=0.5\textwidth]{figures/datasets/coco/samples/val/1.png}%
    }
    \caption{\label{fig:coco-samples}Ground truths for the dataset samples}
\end{figure}


\subsection{Training Settings}
All models are implemented in PyTorch~\cite{Ansel_PyTorch_2_Faster_2024} using an adapted version of the segmentation models repository~\cite{Iakubovskii:2019}. All code and configurations required to train the models and reproduce the experiments can be found on GitHub \footnote[1]{\url{https://github.com/Generative-AI-TUe/msc-project-1297333}}. All models are trained on a NVIDIA Geforce RTX 2080 TI. The gradients are clipped using the norm, with a maximum value of 10. The AdaMax~\cite{kingma2017adammethodstochasticoptimization} is used with a cosine annealing learning rate, varying between $1e^{-3}$ and $1e^{-4}$. Each model is trained on 10.000 minibatches of size 96, unless otherwise specified.

\section{Comparison to baseline architectures}
For the experiments we make use of our method, VAES (ours), FPN, and U-Net. Each model was trained eight times with different configurations. We used different settings for the pre-trained weights:
\begin{itemize}
    \item Without (`None') pre-trained weights.
    \item With pre-trained weights retrieved from classification of ImageNet.
    \item With pre-trained weights retrieved from a $\beta$-vae, with $\beta=1$ and $\beta=100$.
\end{itemize}
Additionally, the model is trained with `frozen' encoder weights (the encoder weights are not updated), and with `unfrozen' encoder weights (the encoder weights are updated). The resulting Evaluation Jaccard Index can be seen in Table~\ref{tab:baseline_results}.

\input{figures/baselines/baselines-results}

An Analysis of Variance (ANOVA) is done, of which the results can be seen in Table~\ref{tab:comparison_baselines_anova_all}. Based on these results an Ordinary Least Squares Linear Model (OLS) is fitted to determine the effect of each significant parameter. The results of this are shown in Table~\ref{tab:comparison_baselines_ols_effects}.

\begin{figure}[h]
    \foreach \i in {0,1,...,4} {
            \centering
            \subfloat[Image]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/VAES-imagenet-False/\i.png}}
            \subfloat[Ground Truth]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/VAES-imagenet-False/gt_\i.png}}
            \subfloat[VAES-imagenet]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/VAES-imagenet-False/pr_\i.png}}
            \subfloat[unet-imagenet]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/unet-imagenet-False/pr_\i.png}}
            \subfloat[fpn-imagenet]{\includegraphics[width=0.2\textwidth]{figures/baselines/samples/fpn-imagenet-False/pr_\i.png}}
            \\
        }
    \caption{Samples of the validation dataset, with the ground truth and the prediction by the model. The encoder is not frozen.}\label{ref:baseline-sample-results-0}
\end{figure}

\input{figures/baselines/tables}

\section{Reduction data required}
To determine if pre-training results in a reduction of labeled training data required, a full factorial design is done over the parameters: architecture type, pre-trained weights, and the percentage of the (labeled) dataset used. The resulting Evaluation Jaccard Index of each model configuration can be seen in Table~\ref{tab:data_fraction_results} and are plotted in Figure~\ref{fig:dataset-fraction-results}.
\input{figures/data_percentage/results_dataset_fraction}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/data_percentage/line-plot.png}
    \caption{Jaccard Index for reduced training dataset size.}
    \label{fig:dataset-fraction-results}
\end{figure}

An Analysis of Variance (ANOVA) is done, of which the results can be seen in Table~\ref{tab:data_fraction_parameter_significance}. Based on these results, it can be determined that there are no significant interaction effects between any of the parameters, and that each individual parameter has a significant effect.
\input{figures/data_percentage/parameter_significance_table}

The effect of the parameters can then be analysed by a simple OLS model. The results of which are shown in Table~\ref{tab:data_fraction_parameter_influence}. It shows that there is a large and significant increase in performance when using the ImageNet weights. The weights gained from pre-training a VAE do not significantly affect the performance, compared to random weights. Furthermore, both the FPN and U-Net architectures are better than the VAES architecture. Finally, the size of the dataset is significant, however it is more important to choose the right architecture and especially pre-trained weights.
\input{figures/data_percentage/parameter_influence_table}

\section{Model Characteristics}

As the computational resources in mobile robots tend to be low, the inference speed and memory usage was measured. Inference was done on a NVIDIA GTX 1070, and measured using PyTorch benchmarking tools. The results can be seen in Table~\ref{tab:model_characteristics}.

\input{figures/model_characteristics}


\section{Ablation Study}
\subsection{Backbone}
We first analyse the influence of the backbone on the performance of the VAE task by comparing the following backbones: MobileViT~\cite{Mehta2022SeparableSF}, MobileNetV2~\cite{sandler2019mobilenetv2invertedresidualslinear}, EfficientNet~\cite{tan2020efficientnetrethinkingmodelscaling} and ResNet50~\cite{he2015deep}. The results can be seen in Table~\ref{tab:backbones-results}. Some examples of the reconstruction capabilities can be seen in Figure~\ref{fig:vae-backbones}.

\begin{figure}[!ht]
    \centering
    \caption{Example reconstruction for the various backbones.}
    \label{fig:vae-backbones}
    \subfloat[Original, EfficientNet]{\includegraphics[width=0.45\linewidth]{figures/vae-backbones/samples/efficientnet_b2/4.png}}
    \subfloat[Original, MobileNetV2]{\includegraphics[width=0.45\linewidth]{figures/vae-backbones/samples/mobilenetv2_100/4.png}} \quad
    \subfloat[Original, MobileViT]{\includegraphics[width=0.45\linewidth]{figures/vae-backbones/samples/mobilevitv2_100/4.png}}
    \subfloat[Original, ResNet50]{\includegraphics[width=0.45\linewidth]{figures/vae-backbones/samples/resnet50/4.png}}
\end{figure}

\input{figures/vae-backbones/backbones_vae.tex}


\subsection{$\beta\text{-value}$}
We also analyse the effect of the $\beta$ factor on the reconstruction quality of the VAE. This is done by training models with varying values for $\beta$. The results can be seen in Table~\ref{tab:beta-vae-loss-values}. Some examples of the reconstruction capabilities can be seen in Figure~\ref{fig:beta-vae-recon-examples}. More samples can be seen in Appendix~\ref{appendix:recon_samples}.

\begin{table}[!ht]
    \centering
    \caption{Loss values resulting from training a Beta-VAE for various $\beta$ values}
    \label{tab:beta-vae-loss-values}
    \begin{tabular}{ccc}
        \hline
        $\beta$ & KL-Divergence & Reconstruction Error ($1e^5$) \\
        \hline
        0.01    & 80200         & 1.9                           \\
        0.1     & 31250         & 1.5                           \\
        1       & 8190          & 2.7                           \\
        10      & 2089          & 2.2                           \\
        100     & 498           & 2.9                           \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[!ht]
    \centering
    \caption{Example reconstructions for $\beta$-vae.}
    \label{fig:beta-vae-recon-examples}
    \subfloat[Original, $\beta$ = 0.01]{\includegraphics[width=0.45\linewidth]{figures/beta-vae/b0.01-0.png}}
    \subfloat[Original, $\beta$ = 0.1]{\includegraphics[width=0.45\linewidth]{figures/beta-vae/b0.1-0.png}} \quad
    \subfloat[Original, $\beta$ = 1]{\includegraphics[width=0.45\linewidth]{figures/beta-vae/b1-0.png}}
    \subfloat[Original, $\beta$ = 10]{\includegraphics[width=0.45\linewidth]{figures/beta-vae/b10-0.png}}
\end{figure}


\subsection{Skip Connection}
To understand the importance of the type and amount of skip connections which are added after the pretraining, we will show the effect of this by incrementally removing more skip connections. We vary the amount of skip connections from 1 to 5 and try all 3 skip connection types. The results can be viewed in Table~\ref{tab:skip_results}.
Again an ANOVA is fitted to determine the significant effects, the results can be seen in Table~\ref{tab:skip_importance_anova_all}. Then, an OLS is fitted to show the effect size of all significant factors, the results of which can be seen in Table~\ref{tab:skip_importance_ols_effects}. From this limited sample of runs the type does not seem to be significant. Neither does the amount of skip connections prove to be a significant effect based on the OLS. However, they are both close to a significance value of $0.05$, thus a bigger sample might show a small (likely positive) effect.

\input{figures/skip_importance/skip-importance-results}

\input{figures/skip_importance/tables}
