\chapter{Introduction}\label{chapter:introduction}

Mobile robotics is a rapidly growing field that has numerous applications in industries such as logistics, agriculture, and healthcare~\cite{cognominal2021evolving,kebede2024review,clark2023amazon}. With the increasing demand for efficient and autonomous mobile robots, researchers have explored various techniques to improve their performance. One of the major challenges is autonomous navigation in unknown environments~\cite{alatise2020review}. It requires the robot to create a map of the environment while traversing it. Another difficulty is properly understanding its position relative to the map. An algorithm that is capable of doing this is Simultaneous Localization and Mapping (SLAM)~\cite{whyte1996slam,whyte2006slam}. It, in theory, solves the problem of exploring an unknown environment, while simultaneously allowing the robot to locate itself in that environment. Thus allowing it to navigate this environment. However, in practice, there still exist many challenges. One of these challenges is the detection of robust landmarks, to which the robot relates its own position as well as the position of other objects that it tries to map. The problem is made even more difficult when the map in which the robot navigates is not stationary, but dynamic. In these dynamic environments, the position of the surrounding objects can also move and thus are not appropriate to be used as landmarks. Moreover, for these robots to quickly react to external influences they need to process all this information in real-time. This is made extra challenging due to the energy and size constraints that many practical applications put on the physical design of the robot.

With the rise of small GPU-accelerated computers, such as the NVIDIA Jetson Orin~\cite{NVIDIA_Karumbunathan_2022}, computer vision has become feasible to run in real-time applications with limited power. This made vision-based SLAM algorithms, which use computer vision to detect landmarks, feasible for mobile robots. One such algorithm is the Dynamic Semantic SLAM~\cite{yu2018ds} (DS-SLAM). It uses SegNet~\cite{badri2017segnet}, a semantic segmentation network, that is able to classify each pixel in a given frame. These classifications can subsequently be used to remove landmarks that are not reliable due to being non-stationary. However, these machine learning algorithms often fail when the data they encounter in production is different from their training data~\cite{Goodfellow-et-al-2016,ozdag2018adversarial,warde201611,10.14778/3632093.3632098,DBLP:journals/corr/KurakinGB16a,10.1145/3422622}. Therefore, collecting data that is representative of the environment in which the robot will operate is important. Although the collection of this raw data can often be done, converting it into a usable dataset on which machine learning algorithms can be trained often requires manual labour. Currently, this is solved by making use of a pre-trained backbone. These backbones are trained on well-known public datasets, such as ImageNet~\cite{deng2009imagenet} for image-related tasks or Common Crawl~\cite{commoncrawl} for text-based tasks. These pre-trained backbones, or foundation models, are then used for fine-tuning on specific tasks. The benefits and risks, are described in great detail by Bommasani et al.~\cite{DBLP:journals/corr/abs-2108-07258}. A few will be highlighted here.

The main benefits of using pre-trained networks are two-fold: they typically improve the convergence speed of the model and reduce the amount of task-specific data required~\cite{donahue2014decaf,zeiler2014visualizing}. Furthermore, many state-of-the-art models in various (image) related tasks, such as object detection~\cite{liu2016ssd,redmon2016you} and semantic segmentation~\cite{orsic2019defense,girshick2014rich} benefit from pre-trained backbones.

However, there are a few major drawbacks. One of these drawbacks is specifically for companies, which cannot use these datasets due to the licencing requirements put on them. A second drawback is that labelled data often contains human bias~\cite{yang2020towards}, which next to the ethical issues it brings, also impacts the performance. Another drawback of these generic datasets is that, in many real-world robotic applications, the actual data distribution encountered is a tiny subset of those seen during training. Thus, the question arises; does a robot tasked with navigating an indoor warehouse benefit from knowing how to detect an elephant or other (big) animals? This is specifically the case with mobile robotics where computational resources are scarce. Thus, by pre-training on the actual dataset, could it avoid learning these useless parts and thereby possibly increase the accuracy on the task at hand? However, these task-specific datasets are often non-existent, and although collecting task-relevant data can often easily be done, e.g., by collecting videos of a warehouse by manually driving a robot around for a day, it remains expensive to convert this raw data into a useful dataset due to the labour required to \emph{properly} label it. In their systematic review of ImageNet, Mishkin et al.~\cite{MISHKIN201711}, show that a smaller high-quality dataset results in better performance compared to a larger low-quality dataset. Therefore, it would be ideal if we could learn most of the important features from the raw data, i.e., unsupervised learning.

One solution is to make use of unsupervised learning methods, which do not require labelled data. One algorithm that can learn from unlabelled data is the Variational Auto Encoder~\cite{kingma2014autoencodingvariationalbayes} (VAE), which will be explained in more detail in Section~\ref{sec:vae}. However, this model is not able to produce (semantic) segmentation masks. This thesis will investigate if it is possible to make use of unsupervised pre-training to reduce the amount of labelled data that is required. Thus reducing the cost of deploying robots to new environments.

\section{Research Questions and Contributions}
This thesis will answer the following research questions:
\begin{enumerate}
    \item Can the VAE be adapted to an Image Segmentation task?
    \item Do the pre-trained weights of a VAE provide useful features for image segmentation?
    \item Does pre-training on the VAE task reduce the amount of labelled data required?
\end{enumerate}
The contributions of this thesis are as follows;
\begin{itemize}
    \item The (semantic) segmentation task is reformulated as a probabilistic model, enabling it to be tackled from a generative modelling perspective.
    \item An adaption of the VAE, the VAE-Segmentation architecture, is proposed. A variational model can be used to produce semantic segmentation masks.
    \item It is shown that using pre-trained weights can increase the performance of semantic segmentation models, having a bigger effect on the final performance compared to the model architecture.
    \item  Furthermore, it is shown that a VAE trained on CoCo does not learn features that are significantly better than those learnt by training from scratch.
\end{itemize}
