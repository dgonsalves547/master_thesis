\chapter{Discussion and Conclusion}\label{chapter:discussion}
In this section chapter the results presented in Chapter~\ref{chapter:experiments} are discussed, after which a conclusion summarizing the complete thesis is drawn. 

\section{Discussion}
In order to perform the comparative study a baseline architecture had to be chosen. Based on the ablation study a smaller backbone could have been chosen as well as a reduced number of skip connections. This could potentially affect the results shown in this thesis. Furthermore, this study is limited to a single dataset, which although containing over 100 thousand images, is tiny compared to the size of ImageNet~\cite{deng2009imagenet} (1 million), Open Images v7 \cite{OpenImages} (1.7 million), and Googles internal JFT-300M~\cite{DBLP:journals/corr/SunSSG17} (300 million) datasets. Similar to transformers, it could be possible that the VAE pre-training only starts to perform better when larger datasets are used, in which more generic features are required to make efficient use of the latent space present. Moreover, all analysis are done on a single run of each configuration, which makes the statistical analysis done in this thesis less accurate. This is especially the case for interaction effects and smaller effects.

Compared to the baseline models the VAES architecture slightly underperformed on the segmentation task, achieving Jaccard Index of $0.05$ lower than U-Net and FPN. Furthermore, the model is more complex. Although it still is capable of processing 50 frames per second on a `old' desktop GPU and the memory requirements are under 2 GB. It performs considerably worse than the baselines, taking up almost 10 times as much memory and being twice as slow to inference a single image.

The pre-trained weights received from our proposed method do not improve the results on the segmentation task. Taking a look at the reconstructions made by the VAE show that in the reconstruction small details are filtered out. This is especially visible for the $\beta=100$-VAE, however it is also noticeable in the other VAEs. This might indicate that the learnt latent space does not involve features for these smaller details. There might exist better VAE structures and loss-functions that could improve the reconstruction, focusing on these finer details. The pre-trained weights of these models could in turn prove to be beneficial.

The usage of pre-trained weights, either received from our proposed VAE method or from the classification task on ImageNet, does not reduce the amount of task specific data that is required. However, it is shown that in the case of the CoCo dataset, only 1 percent of the data is required to achieve top results. This is inline with research on ImageNet by Mishkin et al.~\cite{mishkin2017systematic} that show that there is only a limited performance gain of adding more data and that investing in smaller but high quality datasets is of paramount importance, especially for fine-tuning.

Finally, in the ablation study it is shown that, in contrast to the current development in Large Language Models, smaller models can achieve similar results to more complex and higher parameter count models. These smaller models provide major benefits such as reduced training time, lower memory requirements, and above all faster inference speed. The latter two being especially important in the view of mobile robotics.

\section{Conclusion}\label{chapter:conclusions}

We showed that the Variational Auto-Encoder (VAE) can be adapted to the semantic segmentation task, showing that the semantic segmentation task can be viewed as a generative problem. Its performance is similar to that of a U-Net. However, it is more computationally expensive. Therefore, in its current form, the VAES does not provide any benefits over the simpler U-Net and FPN architectures. This is specifically the case in mobile robotics, in which the inference speed is of huge importance. Here, the slower inference speed of the VAES is a major downside.

Furthermore, it can be concluded that pre-training on the reconstruction task with a VAE does not have a significant effect on the final performance of the model. Therefore, it can be concluded that the features learnt in the VAE on the CoCo dataset are not beneficial to the segmentation task on the CoCo dataset.

Finally, pre-training on the VAE task does not impact the quality of the model when there is only a limited amount of labelled data available. Moreover, using pre-trained weights from a classification task proves to be of vital importance to achieve good results, surpassing the importance of selecting the optimal architecture.
